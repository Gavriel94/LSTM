{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Classification with a Long-Short Term Memory Model\n",
    "\n",
    "The Long-Short Term Memory (LSTM) model is an improvement on the Recurrent Neural Network (RNN) architecture. A RNN processes each element of the input sequentially and updates its parameters using a variation of backpropagation known as backpropagation through time (BPTT). BPTT unrolls the time steps, applies backpropagation and rolls the recurrent structure back up. RNNs are unable to capture a consetual representation of long sequences of data as they have no long term memory. This makes them instable and inefficient as they commonly suffer with vanishing/exploding gradients, halting the learning process.\n",
    "\n",
    "LSTMs work differently. Each LSTM module contains a cell state and a hidden state. The cell state allows a representation of the data to run through the model and undergo updates via linear instructions determined by internal gates. There is a forget gate; used to discard information, an input gate to add new information and an output gate which finalises the state of the module. By maintaining a consistent cell state, gradients flow easily through the network, mitigating the vanishing/exploding gradient problem in most cases. Compared to RNNs, LSTMs have significantly better stability and memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from collections import Counter\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import wandb\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data PreProcessing\n",
    "\n",
    "Each of the six datasets are formatted differently. This cell is used to standardise and strip them of unnecessary data, reducing them to `label` and `text` columns.\n",
    "These datasets are then concatenated to form one large dataset.\n",
    "\n",
    "Data sources:\n",
    "- https://kaggle.com/datasets/kazanova/\n",
    "- https://hasocfire.github.io/hasoc/2021/dataset.html\n",
    "- https://figshare.com/articles/dataset/Labelled_Hate_Speech_Detection_Dataset_/19686954\n",
    "- https://zenodo.org/record/3706866\n",
    "- https://kaggle.com/datasets/ashwiniyer176/toxic-tweets-dataset\n",
    "- https://kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'Data/Unprocessed/training.1600000.processed.noemoticon.csv', \n",
    "    encoding='latin', \n",
    "    header=None\n",
    "    )\n",
    "df.columns = ['label', 'id', 'date', 'query', 'user_id', 'text']\n",
    "df = df.drop(columns=['id', 'date', 'query', 'user_id'])\n",
    "\n",
    "df['label'].mask(df['label'] == 0, 1, inplace=True)\n",
    "df['label'].mask(df['label'] == 2, 0, inplace=True)\n",
    "df['label'].mask(df['label'] == 4, 0, inplace=True)\n",
    "\n",
    "df_2 = pd.read_csv(\n",
    "    'Data/Unprocessed/hasoc_english_dataset.tsv', \n",
    "    delimiter='\\t'\n",
    "    )\n",
    "df_2 = df_2.drop(columns=['text_id', 'task_1', 'task_3'])\n",
    "df_2 = df_2.rename(columns={'task_2': 'label'})\n",
    "\n",
    "df_2['label'].mask(df_2['label'] == 'HATE', 1, inplace=True)\n",
    "df_2['label'].mask(df_2['label'] == 'OFFN', 1, inplace=True)\n",
    "df_2['label'].mask(df_2['label'] == 'PRFN', 1, inplace=True)\n",
    "df_2['label'].mask(df_2['label'] == 'NONE', 0, inplace=True)\n",
    "\n",
    "df_3 = pd.read_csv('Data/Unprocessed/HateSpeechDetection.csv')\n",
    "df_3 = df_3.drop(columns=['Platform'])\n",
    "df_3 = df_3.rename(columns={'Comment': 'text'})\n",
    "df_3 = df_3.rename(columns={'Hateful': 'label'})\n",
    "\n",
    "df_4 = pd.read_csv(\n",
    "    'Data/Unprocessed/hatespeech_text_label_vote_RESTRICTED_100K.csv'\n",
    "    )\n",
    "df_4 = df_4.drop(columns=['Votes for the majority label'])\n",
    "df_4 = df_4.rename(columns={'Tweet text': 'text'})\n",
    "df_4 = df_4.rename(columns={'Label': 'label'})\n",
    "\n",
    "df_4['label'].mask(df_4['label'] == 'normal', 0, inplace=True)\n",
    "df_4['label'].mask(df_4['label'] == 'spam', 0, inplace=True)\n",
    "df_4['label'].mask(df_4['label'] == 'abusive', 1, inplace=True)\n",
    "df_4['label'].mask(df_4['label'] == 'hateful', 1, inplace=True)\n",
    "df_4['text'] = df_4['text'].str.replace('RT', '')\n",
    "\n",
    "df_5 = pd.read_csv('Data/Unprocessed/FinalBalancedDataset.csv')\n",
    "df_5.rename({\"Unnamed: 0\":\"a\"}, axis=\"columns\", inplace=True)\n",
    "df_5.drop([\"a\"], axis=1, inplace=True)\n",
    "df_5 = df_5.rename(columns={'Toxicity': 'label'})\n",
    "df_5 = df_5.rename(columns={'tweet': 'text'})\n",
    "df_5['text'] = df_5['text'].str.replace('ð', '')\n",
    "\n",
    "df_6 = pd.read_csv('Data/Unprocessed/Reddit_Data.csv')\n",
    "df_6 = df_6.rename(columns={'clean_comment': 'text'})\n",
    "df_6 = df_6.rename(columns={'category': 'label'})\n",
    "df_6.drop(df_6[df_6['label'] == 0].index, inplace=True)\n",
    "df_6 = df_6[df_6['text'] != '']\n",
    "df_6['label'].mask(df_6['label'] == 1, 0, inplace=True)\n",
    "df_6['label'].mask(df_6['label'] == -1, 1, inplace=True)\n",
    "\n",
    "\"\"\"\n",
    "Concatenate datasets\n",
    "\"\"\"\n",
    "\n",
    "processed_data = pd.concat([df, df_2, df_3, df_4, df_5, df_6])\n",
    "\n",
    "processed_data['text'] = processed_data['text'].str.replace('https', '')\n",
    "processed_data['text'] = processed_data['text'].str.replace('t', '')\n",
    "processed_data['text'] = processed_data['text'].str.replace('co', '')\n",
    "processed_data['text'] = processed_data['text'].str.replace('amp', '')\n",
    "processed_data['text'] = processed_data['text'].str.replace('quo', '')\n",
    "\n",
    "# save all data\n",
    "processed_data.to_csv('Data/processed_data.csv')\n",
    "print(len(processed_data))\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_words = ' '.join([word for word in processed_data['text'][processed_data['label'] == 0]])\n",
    "\n",
    "neutral_cloud = WordCloud(width=400, height=300, max_font_size=90, max_words=100).generate(neutral_words)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(neutral_cloud, interpolation='bilinear', cmap='plasma')\n",
    "plt.title('Neutral Cloud')\n",
    "plt.axis('off')\n",
    "\n",
    "hate_words = ' '.join([word for word in processed_data['text'][processed_data['label'] == 1]])\n",
    "\n",
    "hate_cloud = WordCloud(width=400, height=300, max_font_size=90, max_words=100).generate(hate_words)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(hate_cloud, interpolation='bilinear', cmap='plasma')\n",
    "plt.title('Hate Cloud')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets\n",
    "\n",
    "The total dataset is very large, containing 1,789,694 items. Training a model on this many items takes a long time so in order to make the training process more manageable the dataset is being split into 8 subsets.\n",
    "Training the model for a long time to discover it overfits of underfits the data is not ideal. The model can be trained on subsets periodically, streamlining the process and enabling better control over how the model updates its parameters.\n",
    "\n",
    "Each subset is split into a training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(data):\n",
    "    \"\"\"\n",
    "    Splits a dataset into train/val/test sets.\n",
    "    \n",
    "    - 81% Training data\n",
    "    - 9% Validation data\n",
    "    - 10% Testing data\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame which needs splitting.\n",
    "\n",
    "    Returns:\n",
    "        train_data, val_data, test_data \n",
    "            (DataFrame, DataFrame, DataFrame): train/va/test sets.\n",
    "    \"\"\"\n",
    "    train_data_ = data.sample(frac=0.9)\n",
    "    test_data = data.drop(train_data_.index)\n",
    "    train_data = train_data_.sample(frac=0.9)\n",
    "    val_data = train_data_.drop(train_data.index)\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def dataset_details(dataset, data_name, set_name):\n",
    "    \"\"\"\n",
    "    Displays the number of elements each class has in the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (DataFrame): \n",
    "        data_name (str): Name of the folder e.g. 'data_1'\n",
    "        set_name (str): Name of the dataset (e.g. 'Training').\n",
    "    \"\"\"\n",
    "    num_samples = len(dataset)\n",
    "    num_label_0 = Counter(dataset['label'].tolist())[0]\n",
    "    num_label_1 = Counter(dataset['label'].tolist())[1]\n",
    "    split_percent = num_label_1 / num_samples * 100\n",
    "    print('*' + '-' * 19 + '*')\n",
    "    print(f'|      {data_name:11}  |')\n",
    "    print(f'|     {set_name:11}   |')\n",
    "    print('|' + '-' * 19 + '|')\n",
    "    print(f'| Samples : {num_samples:7} |')\n",
    "    print('*' + '-' * 19 + '*')\n",
    "    print(f'| Neutral : {num_label_0:6}  |')\n",
    "    print(f'| Hate    : {num_label_1:6}  |')\n",
    "    print(f'| Split   : {split_percent:.2f}%  |')\n",
    "    print('*' + '-' * 19 + '*')\n",
    "    if set_name == 'Testing':\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle dataframe\n",
    "processed_data = processed_data.sample(frac=1)\n",
    "# Split dataframe into 8 subsets\n",
    "split_data = np.array_split(processed_data, 16)\n",
    "# List of subset names\n",
    "data_names = ['data_1', 'data_2', 'data_3', 'data_4', 'data_5', \n",
    "    'data_6', 'data_7', 'data_8', 'data_9', 'data_10', 'data_11',\n",
    "    'data_12', 'data_13', 'data_14', 'data_15', 'data_16']\n",
    "\n",
    "\"\"\"\n",
    "Iterates through the subsets and saves them to disk\n",
    "Creates a training, validation and test set from each subset\n",
    "Displays the label splits and saves each dataset to disk\n",
    "\"\"\"\n",
    "for i in range(len(split_data)):\n",
    "    # reset indexes of each subset and save to disk\n",
    "    split_data[i].reset_index(drop=True, inplace=True)\n",
    "    split_data[i].to_csv(f'Data/Split Datasets/{data_names[i]}.csv')\n",
    "\n",
    "    # Split subset into training, validation and test sets and save\n",
    "    train, val, test = train_val_test_split(split_data[i])\n",
    "    dataset_details(train, data_name=data_names[i], set_name='Training')\n",
    "    dataset_details(val, data_name=data_names[i], set_name='Validation')\n",
    "    dataset_details(test, data_name=data_names[i], set_name='Testing')\n",
    "    train.to_csv(f'Data/Split Datasets/{data_names[i]}/train.csv')\n",
    "    val.to_csv(f'Data/Split Datasets/{data_names[i]}/val.csv')\n",
    "    test.to_csv(f'Data/Split Datasets/{data_names[i]}/test.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate Batches and Initialise DataLoaders\n",
    "\n",
    "Collates `label`/`text` pairs into tuples, where the text is transformed into its GloVe embedding. Sequences are padded and tensors are moved to the specified device. \n",
    "\n",
    "Batches are padded so they are all the same length. \n",
    "\n",
    "Used by DataLoaders to shuffle data each epoch and process the data using the pipeline described above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch, embeddings, dimensions, device):\n",
    "    \"\"\"\n",
    "    Custom collate function to prepare batches for LSTM processing.\n",
    "    Ensures the label is an integer and transforms the input sequence\n",
    "        to its vector embedding.\n",
    "    \n",
    "    Args:\n",
    "        batch (numpy.array): List of tuples containing label/text pairs.\n",
    "        embeddings (dict): Dictionary of word/vector embedding pairs.\n",
    "        dimensions (int): Dimensions used for embedding vector.\n",
    "        device (torch.device): Device used to process tensors.\n",
    "\n",
    "    Returns:\n",
    "        label_list, text_list, text_lengths \n",
    "        (torch.Tensor, torch.Tensor, torch.Tensor): \n",
    "            Tensors of labels, padded embeddings and text lengths.\n",
    "    \"\"\"\n",
    "    # Containers for labels and text embeddings.\n",
    "    label_list, text_list = [], []\n",
    "    for (label, text) in batch:\n",
    "        # Stores the label as an integer\n",
    "        label_list.append(int(label))\n",
    "        embedding = []\n",
    "        for word in text.split():\n",
    "            # If word doesn't exist return vector of 0's.\n",
    "            vector = embeddings.get(word, np.zeros((dimensions,)))\n",
    "            # Convert embedding to tensor.\n",
    "            embedding.append(torch.tensor(vector, dtype=torch.float32))\n",
    "        # Store embedded sequence in text_list\n",
    "        text_list.append(torch.stack(embedding))\n",
    "    # Convert labels to tensors.\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    # Pad all sequences to same as longest sequence length.\n",
    "    text_list = rnn_utils.pad_sequence(text_list, batch_first=True)\n",
    "    # Create tensor storing length of sequences.\n",
    "    text_lengths = torch.tensor([len(t) for t in text_list], dtype=torch.int64)\n",
    "    # Move data to specified device (GPU/CPU).\n",
    "    return label_list.to(device), text_list.to(device), text_lengths.to(device)\n",
    "\n",
    "\n",
    "def batch_padding(batch_size, embeddings, dimensions, device):\n",
    "    \"\"\"\n",
    "    Pads batches to specified size then processes using `collate_fn()`. \n",
    "    Pads by repeating the last element until batch_size is reached.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): The size of each batch.\n",
    "        embeddings (dict): Dictionary of word embeddings.\n",
    "    \n",
    "    Returns:\n",
    "        collate_fn (function): Function to pad a batch and \n",
    "            processes it `collage_batch()`\n",
    "    \"\"\"\n",
    "    def collate_fn(batch, dimensions=dimensions):\n",
    "        padded_batch = batch + [batch[-1]] * (batch_size - len(batch))\n",
    "        return collate_batch(padded_batch, embeddings, dimensions, device)\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary and DataLoaders\n",
    "\n",
    "GloVe (Global Vectors for Word Representation) embeddings are dense vector\n",
    "representations of words. Vector values represent a multidimensional map of semantic, syntactic and other linguistic attributes of words.\n",
    "\n",
    "Returns the embeddings as a dictionary of word/vector pairs.\n",
    "Defines process of creating DataLoaders from a dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    \"\"\"\n",
    "    Device used to process tensors.\n",
    "    \"\"\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        if torch.backends.mps.is_built():\n",
    "            print('Using MPS')\n",
    "            return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def glove_embedding_dict(file):\n",
    "    \"\"\"\n",
    "    Produces a dictionary of word/GloVe embeddings.\n",
    "\n",
    "    Args:\n",
    "        file (str): Path to GloVe embeddings file.\n",
    "\n",
    "    Returns:\n",
    "        embedding_dict (dict): word/GloVe embeddings.\n",
    "    \"\"\"\n",
    "    embedding_dict = {} \n",
    "\n",
    "    with open(file, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embedding_dict[word] = vector\n",
    "    return embedding_dict\n",
    "\n",
    "def init_dataloaders(folder, \n",
    "                     embedding_dict, \n",
    "                     batch_size, \n",
    "                     dimensions, \n",
    "                     device):\n",
    "    \"\"\"\n",
    "    Creates train/val/test DataLoaders.\n",
    "\n",
    "    Args:\n",
    "        folder (str): train/val/test csv within 'Data/Split Datasets/'. \n",
    "        embedding_dict (dict): word/GloVe embeddings\n",
    "        batch_size (int): Batch size\n",
    "        dimensions (int): Dimensions of data. Used for padding batches.\n",
    "        device (torch.device): Device used to process data.\n",
    "\n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader\n",
    "            (DataLoader, DataLoader, DataLoader): Required DataLoaders.\n",
    "    \"\"\"\n",
    "    train = pd.read_csv(\n",
    "        f'Data/Split Datasets/{folder}/train.csv'\n",
    "        ).drop(columns=['Unnamed: 0'])\n",
    "    val = pd.read_csv(\n",
    "        f'Data/Split Datasets/{folder}/val.csv'\n",
    "        ).drop(columns=['Unnamed: 0'])\n",
    "    test = pd.read_csv(\n",
    "        f'Data/Split Datasets/{folder}/test.csv'\n",
    "        ).drop(columns=['Unnamed: 0'])\n",
    "\n",
    "    train = train.to_numpy()\n",
    "    val = val.to_numpy()\n",
    "    test = test.to_numpy()\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=batch_padding(batch_size, embedding_dict, dimensions, device)\n",
    "        )\n",
    "        \n",
    "    val_loader = DataLoader(\n",
    "        val, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=batch_padding(batch_size, embedding_dict, dimensions, device)\n",
    "        )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=batch_padding(batch_size, embedding_dict, dimensions, device)\n",
    "        )\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions\n",
    "\n",
    "A training function allows the model to make predictions and 'learn' through trial and error.\n",
    "The evaluation function is used to asses the models performance on unseen data. The model is unable to learn from this data so it's used to repeatedly test the model.\n",
    "\n",
    "An evaluation is performed after every epoch (pass through the training data) so changes in perfomance can be seen in real time.\n",
    "\n",
    "`plot_metrics` is defined which displays the metrics visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(t_metric, v_metric, metric, num_epochs):\n",
    "    \"\"\"\n",
    "    Displays accuracy and validation metrics plotted on line graphs.\n",
    "\n",
    "    Args:\n",
    "        t_metric (list): Accuracy/loss values during training.\n",
    "        v_metric (list): Accuracy/loss values during validation.\n",
    "        metric (str): Type of metric being plotted\n",
    "        num_epochs (int): Number of epochs the model was trained.\n",
    "    \"\"\"\n",
    "    plt.plot(t_metric, c='blue', label=f'Training')\n",
    "    plt.plot(v_metric, c='red', label=f'Validation')\n",
    "    plt.ylabel(f'{metric.title()}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title(f'{metric.title()} over {num_epochs} epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def train(dataloader, \n",
    "          model, \n",
    "          optimizer, \n",
    "          scheduler, \n",
    "          criterion, \n",
    "          epoch, \n",
    "          verbose=True):\n",
    "    \"\"\"\n",
    "    Used to train a neural network.\n",
    "    Sets the model to training mode and initialises variables to keep \n",
    "    track of metrics during training.\n",
    "    Iterates through label/text pairs from each dataset making label\n",
    "    predictions. \n",
    "    Calculates loss and backpropagates parameter updates\n",
    "    through the network to ideally reduce loss over epochs.\n",
    "\n",
    "    Args:\n",
    "        dataloader (DataLoader): DataLoader containing training\n",
    "            data.\n",
    "        model (nn.Module): The LSTM model being trained.\n",
    "        optimizer (torch.optim.sgd): Backpropagation method.\n",
    "        criterion (torch.nn.modules.loss): Loss function.\n",
    "        epoch (int): The current epoch.\n",
    "        verbose (Boolean): Display metrics (default=True).\n",
    "\n",
    "    Returns:\n",
    "        epoch_loss, epoch_accuracy, epoch_count \n",
    "            (float, float, int): loss, accuracy and number of \n",
    "            predictions made in one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    # Accuracy and loss of each prediction\n",
    "    accuracy, loss = 0, 0\n",
    "    # Accuracy and loss accumulated over epoch\n",
    "    total_accuracy, total_loss = 0, 0\n",
    "    # Number of predictions\n",
    "    num_predictions = 0\n",
    "    # Displays training metrics every quarter of epoch\n",
    "    intervals = (len(dataloader) / 4).__round__()\n",
    "    for idx, (label, text, text_lengths) in enumerate(dataloader):\n",
    "        # Make prediction\n",
    "        prediction = model(text, text_lengths)\n",
    "        label = label.unsqueeze(1)\n",
    "        # Calculate loss\n",
    "        loss = criterion(prediction, label.float())\n",
    "        batch_loss = loss.item()\n",
    "        # Update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Store metrics\n",
    "        accuracy += ((prediction > 0.5) == label).sum().item()\n",
    "        total_loss += batch_loss\n",
    "        num_predictions += label.size(0)\n",
    "        total_accuracy += accuracy\n",
    "\n",
    "        if verbose and idx % intervals == 0 and idx > 0:\n",
    "            epoch_metrics = (\n",
    "                f'| Epoch {epoch + 1} |' \n",
    "                f'{idx:5} / {len(dataloader):5} batches |' \n",
    "                f'{(accuracy/num_predictions)*100:.10f}% accurate |'\n",
    "                )\n",
    "            print(epoch_metrics)\n",
    "            accuracy = 0\n",
    "            num_predictions = 0\n",
    "    scheduler.step()\n",
    "    return total_loss, total_accuracy, num_predictions\n",
    "\n",
    "def evaluate(dataloader, model, criterion):\n",
    "    \"\"\"\n",
    "    Used to evaluate model training.\n",
    "    Works similarly to the training method, allowing the model\n",
    "    to make predictions on labelled data, however no parameters are\n",
    "    updated.\n",
    "\n",
    "    Args:\n",
    "        dataloader (DataLoader): DataLoader containing either validation\n",
    "            or testing data.\n",
    "        model (nn.Module): The LSTM model being trained.\n",
    "        criterion (torch.nn.modules.loss): Loss function.\n",
    "\n",
    "    Returns:\n",
    "        batch_loss, batch_accuracy, batch_count \n",
    "            (float, float, int): loss, accuracy and number of \n",
    "            predictions made over the validation set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_accuracy = 0\n",
    "    total_count = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, text_length) in enumerate(dataloader):\n",
    "            prediction = model(text, text_length)\n",
    "            label = label.unsqueeze(1)\n",
    "            loss = criterion(prediction, label.float())\n",
    "            total_accuracy += ((prediction > 0.5) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss, total_accuracy, total_count\n",
    "\n",
    "def model_env(training, \n",
    "              validation, \n",
    "              testing, \n",
    "              model, \n",
    "              optimizer,\n",
    "              scheduler, \n",
    "              criterion, \n",
    "              epochs, \n",
    "              verbose=True):\n",
    "    \"\"\"\n",
    "    Wraps the training and evaluation functions in one method.\n",
    "    At the end of each epoch, the model asseses the validation set.\n",
    "    Once all epochs are complete performance is assesed on the test set.\n",
    "\n",
    "    Args:\n",
    "        training (DataLoader): DataLoader with training data.\n",
    "        validation (DataLoader): DataLoader with validation data.\n",
    "        testing (DataLoader): DataLoader with testing data.\n",
    "        model (nn.Module): The LSTM model being trained.\n",
    "        optimizer (torch.optim.sgd): Backpropagation method.\n",
    "        criterion (torch.nn.modules.loss): Loss function.\n",
    "        epochs (int): Number of epochs the model is trained for.\n",
    "        verbose (Boolean): Display metrics (default=True).\n",
    "\n",
    "    Returns:\n",
    "        train_accuracy, train_loss, val_accuracy, val_loss \n",
    "            (list, list, list, list): Metrics saved during training and\n",
    "            evaluation.\n",
    "    \"\"\"\n",
    "    # Containers for training and evaluation metrics\n",
    "    train_accuracy = []\n",
    "    train_loss = []\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    # Time saved for calculating final processing time\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print('-' * 59)\n",
    "        print(f'|\\t\\t          Epoch {epoch + 1}           \\t\\t  |')\n",
    "        print('-' * 59)\n",
    "        # Process training data\n",
    "        t_loss, t_acc, count = train(training, \n",
    "                                 model, \n",
    "                                 optimizer,\n",
    "                                 scheduler, \n",
    "                                 criterion, \n",
    "                                 epoch, \n",
    "                                 verbose)  \n",
    "        # Store training metrics\n",
    "        train_loss.append(t_loss)\n",
    "        train_accuracy.append(t_acc)\n",
    "        # Evaluate validation data\n",
    "        v_loss, v_acc, count = evaluate(validation, model, criterion)\n",
    "        # Store evaluation metrics\n",
    "        val_loss.append(v_loss)\n",
    "        val_accuracy.append(v_acc)\n",
    "        val_ratio = (v_acc/count)*100\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "        'Epoch': epoch,\n",
    "        'Training Accuracy': t_acc,\n",
    "        'Training Loss': t_loss,\n",
    "        'Validation Accuracy': v_acc, \n",
    "        'Validation Loss': v_loss,\n",
    "        })\n",
    "\n",
    "        print('-' * 59)\n",
    "        epoch_metrics = (\n",
    "            f'| End of epoch {epoch + 1} |'\n",
    "            f'Time: {time.time() - epoch_start:.2f}s|'\n",
    "            f'Acc: {val_ratio:.14f}%  |'\n",
    "        )\n",
    "        print(epoch_metrics)\n",
    "        print('-' * 59)\n",
    "        print()\n",
    "\n",
    "    # Assess model performance on test data\n",
    "    wandb.finish()\n",
    "    loss, acc, count = evaluate(testing, model, criterion)\n",
    "    test_ratio = (acc/count)*100\n",
    "    total_time = (time.time() - start_time).__round__()/60\n",
    "    print('*' + '-' * 57 + '*')\n",
    "    test_metrics = (\n",
    "        f'*\\t\\t\\tTesting Epoch\\t\\t\\t  *\\n'\n",
    "        f'*' + '-' * 57 + '*\\n'\n",
    "        f'* \\t\\t      Time: {total_time:.2f} minutes\\t\\t  *\\n'\n",
    "        f'*\\t\\t    Acc: {test_ratio:.14f}%     \\t  *'\n",
    "    )\n",
    "    print(test_metrics)\n",
    "    print('*' + '-' * 57 + '*')\n",
    "    return train_accuracy, train_loss, val_accuracy, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model used to process and predict sentiment of\n",
    "        a textual input sequence.\n",
    "    Extracts data from an input sequence via LSTM modules before the \n",
    "        classifier determines an output.\n",
    "\n",
    "    Args:\n",
    "        nn (nn.Module): Base class for PyTorch neural networks.\n",
    "    \"\"\"\n",
    "    def __init__(self, vector_dim, num_hidden_nodes, hidden_layers):\n",
    "        \"\"\"\n",
    "        Initialises LSTM modules, the classifier, \n",
    "            batch normalisation, ReLU activation and dropout.\n",
    "\n",
    "        Args:\n",
    "            vector_dim (int): Dimensions of vector embeddings.\n",
    "            num_hidden_nodes (int): Number of nodes in the LSTM module.\n",
    "            hidden_layers (int): Number of hidden layers in the module.\n",
    "        \"\"\"\n",
    "        super(LSTM_Model, self).__init__()\n",
    "\n",
    "        self.lstm1 = nn.LSTM(vector_dim,\n",
    "                    num_hidden_nodes,\n",
    "                    hidden_layers,\n",
    "                    bidirectional=True,\n",
    "                    dropout=0.2,\n",
    "                    batch_first=True)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(num_hidden_nodes*2,\n",
    "                    num_hidden_nodes*4,\n",
    "                    hidden_layers,\n",
    "                    bidirectional=True,\n",
    "                    dropout=0.2,\n",
    "                    batch_first=True)\n",
    "\n",
    "        self.lstm3 = nn.LSTM(num_hidden_nodes*8,\n",
    "                    num_hidden_nodes*5,\n",
    "                    hidden_layers,\n",
    "                    bidirectional=True,\n",
    "                    dropout=0.2,\n",
    "                    batch_first=True)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_hidden_nodes*10, 320),\n",
    "            nn.BatchNorm1d(320),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Linear(320, 160),\n",
    "            nn.BatchNorm1d(160),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(160, 80),\n",
    "            nn.BatchNorm1d(80),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(80, 40),\n",
    "            nn.BatchNorm1d(40),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(40, 24),\n",
    "            nn.BatchNorm1d(24),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(24, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, text, text_lengths):\n",
    "        \"\"\"\n",
    "        Determines the order of operations in the model.\n",
    "        Processes the text through LSTM modules and a classifier.\n",
    "\n",
    "        Args:\n",
    "            text (torch.Tensor): Tensor containing embedded sequences.\n",
    "                Shape: [batch_size, seq_length, embedding_dim]\n",
    "            text_lengths (torch.Tensor): Tensor representing the length \n",
    "                of each text sequence in the batch. \n",
    "                Shape: [batch_size]\n",
    "\n",
    "        Returns:\n",
    "            x (torch.Tensor): A tensor of the model's predictions. \n",
    "        \"\"\"\n",
    "        x, _ = self.lstm1(text)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = x[torch.arange(x.shape[0]), text_lengths-1, :]\n",
    "        x = self.classifier(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameters and Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings must be the same size as `dimensions`\n",
    "glove_embeddings = glove_embedding_dict(\n",
    "    file='Data/GloVe/glove.twitter.27B.100d.txt')\n",
    "\n",
    "# ! These three parameters must be the same when loading a model\n",
    "\n",
    "# Must be the same dimensions as GloVe embeddings\n",
    "dimensions = 100\n",
    "# Number of nodes in the LSTM module\n",
    "hidden_nodes = 64\n",
    "# Number of layers in the LSTM module\n",
    "hidden_layers = 3\n",
    "# Size of batches \n",
    "batch_size = 128\n",
    "# Set device used to process tensors\n",
    "device = set_device()\n",
    "\n",
    "# Initialise DataLoaders from Data/Split Datasets/\n",
    "train_loader, val_loader, test_loader = init_dataloaders(\n",
    "    folder='data_1',\n",
    "    embedding_dict=glove_embeddings,\n",
    "    batch_size=batch_size,\n",
    "    dimensions=dimensions,\n",
    "    device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_ = LSTM_Model(\n",
    "    vector_dim=dimensions, \n",
    "    num_hidden_nodes=hidden_nodes, \n",
    "    hidden_layers=hidden_layers\n",
    "    ).to(device)\n",
    "\n",
    "# Compile from PyTorch 2.0\n",
    "# torch._dynamo.reset()\n",
    "lstm = torch.compile(lstm_, backend='eager')\n",
    "\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(lstm.parameters(), lr=learning_rate, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                      step_size=5,\n",
    "                                      gamma=0.8)\n",
    "\n",
    "params = sum(p.numel() for p in lstm.parameters())\n",
    "# used to make parameter number more readable\n",
    "if len(str(params)) == 7:\n",
    "    params = str(params)[:1] + ',' + str(params)[1:]\n",
    "    params = params[:5] + ',' + params[5:]\n",
    "if len(str(params)) == 8:\n",
    "    params = str(params)[:2] + ',' + str(params)[2:]\n",
    "    params = params[:6] + ',' + params[6:]\n",
    "print(f\"LSTM has {params} trainable parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "\n",
    "wandb.init(\n",
    "    project = \"hate-speech-classification\",\n",
    "    config = {\n",
    "        \"learning rate\": learning_rate,\n",
    "        \"architecture\": \"LSTM\",\n",
    "        \"epochs\": epochs,\n",
    "    }\n",
    ")\n",
    "\n",
    "t_acc, t_loss, v_acc, v_loss = model_env(\n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    test_loader, \n",
    "    lstm, \n",
    "    optimizer,\n",
    "    scheduler, \n",
    "    criterion, \n",
    "    epochs, \n",
    "    verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(t_acc, v_acc, 'Accuracy', epochs)\n",
    "plot_metrics(t_loss, v_loss, 'Loss', epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "This cell saves the current state of the model and the optimizer dictionary. \n",
    "This allows the model to be trained with new data at different times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "\n",
    "Load the model to resume training or make predictions.\n",
    "If training the model, ensure the data folder is correct when creating new DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_optimizer):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': model_optimizer.state_dict(),\n",
    "    }, 'checkpoint.pth')\n",
    "    \n",
    "def load_model():\n",
    "    if torch.backends.mps.is_available():\n",
    "        if torch.backends.mps.is_built():\n",
    "            print(\"Using MPS\\n\")\n",
    "            device = torch.device(\"mps\")\n",
    "    else:\n",
    "        raise ValueError('Model cannot train without MPS device\\n')\n",
    "\n",
    "# Embeddings must be the same size as `dimensions`\n",
    "glove_embeddings = glove_embedding_dict(\n",
    "    file='Data/GloVe/glove.twitter.27B.100d.txt')\n",
    "\n",
    "# ! These parameters must be the same as the saved model\n",
    "# Must be the same as GloVe embeddings\n",
    "dimensions = 100\n",
    "# Number of nodes in the LSTM module\n",
    "hidden_nodes = 5\n",
    "# Number of layers in the LSTM module\n",
    "hidden_layers = 4\n",
    "\n",
    "lstm_ = LSTM_Model(\n",
    "    vector_dim=dimensions, \n",
    "    num_hidden_nodes=hidden_nodes, \n",
    "    num_layers=hidden_layers\n",
    "    ).to(device)\n",
    "\n",
    "# Using PyTorch's compile method\n",
    "lstm = torch.compile(lstm_, backend='inductor')\n",
    "\n",
    "# ! These must be the same as the saved model\n",
    "optimizer = optim.SGD(lstm.parameters(), lr=0.1, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, \n",
    "                                      step_size=8,\n",
    "                                      gamma=0.8)\n",
    "\n",
    "checkpoint = torch.load('checkpoint.pth')\n",
    "lstm.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "glove_embeddings = glove_embedding_dict()\n",
    "\n",
    "\"\"\"\n",
    "Model has currently been trained on:\n",
    "☑ data_1\n",
    "☐ data_2\n",
    "☐ data_3\n",
    "☐ data_4\n",
    "☐ data_5\n",
    "☐ data_6\n",
    "☐ data_7\n",
    "☐ data_8\n",
    "\"\"\"\n",
    "# * Ensure this is the correct folder before training\n",
    "data_folder = 'data_2'\n",
    "\n",
    "train_loader, val_loader, test_loader = init_dataloaders(\n",
    "    folder=data_folder, \n",
    "    embedding_dict=glove_embeddings,\n",
    "    batch_size=128\n",
    "    )\n",
    "\n",
    "epochs = 80\n",
    "\n",
    "t_acc, t_loss, v_acc, v_loss = model_env(\n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    test_loader, \n",
    "    lstm, \n",
    "    optimizer,\n",
    "    scheduler, \n",
    "    criterion, \n",
    "    epochs, \n",
    "    verbose=True\n",
    "    )\n",
    "\n",
    "plot_metrics(t_acc, v_acc, 'Accuracy', epochs)\n",
    "plot_metrics(t_loss, v_loss, 'Loss', epochs)\n",
    "\n",
    "print(t_acc, v_acc, t_loss, v_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
